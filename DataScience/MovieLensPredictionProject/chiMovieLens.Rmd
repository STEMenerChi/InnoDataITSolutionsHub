---
title: "Movie Recommendation - Machine Learning Project"
subtible: "HarvardX PH125.9x Data Science Capstone"
author: "C.T. Dinh"
date: "`r Sys.Date()`"
output:
  html_document:
  df_print: paged
  pdf_document: default
  word_document: default
urlcolor: blue
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::read_chunk("chiMovieLens.R")
```

```{r Required Packages, message=FALSE, warning=FALSE, include=FALSE}

if(!require(broom)) install.packages("broom", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table")
if(!require(dplyr)) install.packages("dplyr")
if(!require(dslabs)) install.packages("dslabs")
if(!require(gghighlight)) install.packages("gghighlight")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(ggrepel)) install.packages("ggrepel", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(pagedown)) install.packages("pagedown", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr")
if(!require(readxl)) install.packages("readxl")
if(!require(scales)) install.packages("scales")
if(!require(stringr)) install.packages("stringr")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")

tidyverse_conflicts()  #lists all the conflicts between packages in the tidyverse and other loaded packages. 

library(broom)   # broom and kableExtra packages produce beautiful tables
library(caret)
library(data.table)
library(dplyr)   # for data manipulation functions, filter, select, sort, delete, aggregate
library(dslabs)  
library(gghighlight)
library(ggplot2)
library(ggrepel)  # to use geom_text_repel to prevent text overlapping on graph
library(ggthemes)
library(kableExtra)
library(pagedown) # to convert from html to pdf
library(readr)
library(readxl)   # to import excel data
library(scales)   # to scale y-axis, for example from 8e+05 to 800,000
library(stringr) 
library(tidyr)   # tidyr is the Tidyverse package to help create tidy data
library(tidyverse)
```
# Introduction 

The objective of this project is to develop a movie recommendation system using machine learning algorithm.
The first step is to exam the data, visualize it, and then progressively develop a model that will satisfy 
the target accuracy of the residual mean squared error (RMSE) less than **0.8649.**

More details on [Recommendation systems](http://rafalab.dfci.harvard.edu/dsbook/large-datasets.html#user-effects).

The R Script, R Markdown, and HTML and PDF output reports of this project can be accessed via [Github](https://github.com/STEMenerChi/DataScience/tree/main/HarvardXMovieLensProject).

Software & Hardware Capacity: R v4.2.2 running on i7 Intel Core at 1.90/2.11 GHz CPU and 16GB RAM laptop.
** The total time to run this project is approximately 1 to 1.5 hours.**

# Background 
In October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% 
and win a million dollars.In September 2009, the winners were announced. A summary of how the winning algorithm was put together can be found [here](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/). 

Some of the winning team's data analysis strategies will be explored in this project.

# MovieLens Dataset
The Netflix data is not publicly available, but the GroupLens research lab generated their own database with over 20 million ratings 
for over 27,000 movies by more than 138,000 users. 

For this project the [10M MovieLensDataset Version](http://files.grouplens.org/datasets/movielens/ml-10m.zip) from 
[groupLens](https://grouplens.org/datasets/movielens/10m/) will be used. It contains 10 million ratings on 10,000 movies by 72,000 users.   

The 10M MovieLens Dataset will be partitioned into two datasets: edx and validation (final_holdout_test). 
The edx dataset will be partitioned further into train and test datasets 90/10 ratio respectively. 

The modeling approaches will be developed and evaluated using the edx partitions (train and test). 
The model with the best accuracy will be tested using the validation set (final_holdout_test). 

# Data Loading 
**This process may take a few minutes to run**. 
```{r Data Loading, echo=TRUE, message=FALSE, warning=FALSE}
#####################################
# Data Loading
# Create edx dataset and final_holdout_test as a validate dataset
#####################################
options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# final_holdout_test set contains 10% (p.01) of MovieLens data and serves as validation dataset. 
set.seed(1) 
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final_holdout_test set are also in edx set
# Semi_join() function to return in one data frame that have matching values in another data frame. 
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

validation <- final_holdout_test  # Assign final_holdout_test to validation object 

# Add rows removed from final_holdout_test set back into edx set
# anti_join() function from the dplyr package to return all rows in one data frame that do not have matching values in another data frame.
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

# rm() function to delete objects no longer required from the memory to reduce memory footprint
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

# Exploratory Data Analysis (EDA) - MovieLens Dataset Overview

The edx dataset shows ~9 millions rows of observations and six(6) columns of variables.
Each row of observation represents a rating given per user per movie.

  - Six variables are userID,  movieID, rating, timestamp, title, and genres. 
  - The userID, movieID, and timestamp (in seconds) are the whole numeric values of integer.
  - The movieID and userId are the focal data points for the models.  
  - The string title contains the release year and it can be split from the title if it's useful for prediction.
  - Genres are pipe-delimited string containing 18 unique genre categories including Action, Comedy, Sci-Fi, etc.
    The genre categories can be split if it affects rating outcome or useful for prediction.  
     
Examine edx data structure:
```{r Edx data structure, echo=TRUE, message=FALSE, warning=FALSE}
 str(edx)

```
Edx first seven rows: 
```{r First 7 Rows of edx, echo=FALSE, message=FALSE, warning=FALSE}
edx[1:7, ] %>%  # show all cols
kbl() %>%
  kable_paper("hover", full_width = F)
```

Number of unique users and movies: 
```{r Unique Users and Moviees, echo=TRUE, message=FALSE, warning=FALSE}
edx %>% summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId))
```

There are 10 different ratings lowest is 0.5 and highest 5.0. The average is 3.51, and there are no zero ratings. 
```{r Ratings, echo=FALSE, message=FALSE, warning=FALSE}
unique(edx$rating)
mean(edx$rating)

```

Examine training distribution of the ratings further, 4.0 seems to be the most given rating. 

```{r Rating, message=FALSE, warning=FALSE}
  edx %>%
    ggplot(aes(rating, y = ..prop..)) +
    geom_bar(color = "darkblue", fill="lightblue") +
    labs(x = "Ratings", y = "Proportion") +
    ggtitle("Rating Distribution (Training)") +
    theme(plot.title = element_text(size=12)) +
    scale_x_continuous(breaks = c(0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5))
```

A histogram is one of the popular graphing tools in data science. It helps to visualize whether the distribution is symmetric or skewed left or right. 
It can also show any outliners or gaps in the data. 

The Movies Rated histogram shows that some movies are rated more than others given that there are blockbuster movies watched by millions
and artsy, independent movies watch by just a few per[33.7.1 Movielens data](http://rafalab.dfci.harvard.edu/dsbook/large-datasets.html#movielens-data).

```{r Movies Rated, echo=TRUE, message=FALSE, warning=FALSE}
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, binwidth=0.2,  show.legend = FALSE, aes(fill = cut(n, 100)), fill="lightblue", color="darkblue") + 
  scale_x_log10() + 
  ggtitle("Movies Rated") +
  xlab("Times Rated") +
  ylab("Movie Count")


```

The following graph shows Top 10 most rated movies.

```{r Top 10 Movies, echo=TRUE, message=FALSE, warning=FALSE}
edx %>%
  group_by(title) %>%
  summarize(count = n()) %>%
  arrange(-count) %>%
  top_n(10, count) %>%
  ggplot(aes(count, reorder(title, count))) +
  geom_bar(color = "darkblue", fill="lightblue", stat = "identity") +
  ggtitle("Top 10 Rated Movies") +
  xlab("Times Rated") +
  ylab("")

```

The majority of users rate between 10 and 100 movies, while some rate over 1,000. The graph shows a right skew in its distribution. This implies that some users are more active than other at rating movies.

```{r User Rated, echo=TRUE, message=FALSE, warning=FALSE}
  edx %>% count(userId) %>% 
    ggplot(aes(n)) + 
    geom_histogram(bins = 40, binwidth=0.2, show.legend = FALSE, aes(fill = cut(n, 100)), fill="lightblue", color="darkblue") + 
    scale_x_log10() + 
    ggtitle("User Reviews") +
    xlab("Ratings") +
    ylab("Users")
```

Number of unique genres: 
```{r Unique Genres, echo=FALSE, message=FALSE, warning=FALSE}
edx %>% summarize(n_genres = n_distinct(genres))
```

Top 20 genres ordered by number of ratings in this dataset. 
```{r Top 20 Genres, echo=FALSE, message=FALSE, warning=FALSE}

top20genres <- edx %>% group_by(genres) %>% 
    summarise(count=n()) %>% 
    arrange(-count) %>%
    top_n(20, count)
  
top20genres[1:20, ] %>% #show all 20 rows
kbl() %>%
kable_paper("hover", full_width = F)
```

There was an exponential growth of the movie count especially in  1995 and a sudden drop in 2010.
The reason for the drop was that the data was not collected until Oct 2009;therefore, we don't have the full data on this year. 

Note that the growth of the internet didn't start until 1985. There was barely any rating prior to 1985 due to lack of internet and public access. 

```{r Rating Trend Based on Releaseyear, echo=TRUE, message=FALSE, warning=FALSE}
# Exam the movie release years
# Extract release year from string title into a separate numeric field
  edx <- edx %>% mutate(releaseyear = as.numeric(str_extract(str_extract(title, "[/(]\\d{4}[/)]$"), regex("\\d{4}"))),title = str_remove(title, "[/(]\\d{4}[/)]$")) 

# Number of movies per year/decade
 movies_per_year <- edx %>%
    select(movieId, releaseyear) %>% # select columns we need
    group_by(releaseyear) %>%        # group by year
    summarise(count = n())  %>%      # count movies per year
    arrange(releaseyear)
  
# Exam the rating trend based on releaseyear 
movies_per_year %>%
  ggplot(aes(x = releaseyear, y = count)) +
  scale_y_continuous(labels = comma) +
  geom_line(color="blue")
  
```

Older movies seem to get higher rating.

```{r Release Year vs Avg Rating, echo=TRUE, message=FALSE, warning=FALSE}
 
  # Exam release year vs ratings mean
  edx %>% group_by(releaseyear) %>%
    summarize(rating = mean(rating)) %>%
    ggplot(aes(releaseyear, rating)) +
    geom_point() +
    theme_hc() + 
    geom_smooth() +
    theme_bw()+ 
    ggtitle("Release Year vs. Rating")

 
```

It would be difficult to factor genre into the overall prediction because certain genres being more popular in certain periods.

# Data Wrangling 
Partition edx dataset further into train and test sets 90/10 ratio respectively. 

```{r Data Wrangling, echo=TRUE, message=FALSE, warning=FALSE}
########################################################################################
# Data Wrangling 
#########################################################################################
 set.seed(1)  # This is a randomized algorithm 
 test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
 train <- edx[-test_index,]
 temp <- edx[test_index,]
  
  # Make sure userId and movieId in test set are also in train set. 
  test <- temp %>% 
    semi_join(train, by = "movieId") %>%  
    semi_join(train, by = "userId")
  
  # Add rows removed from test set back into train set
  removed <- anti_join(temp, test)
  train <- rbind(train, removed)
  
  rm(test_index, temp, removed)
```

# Predictive Model Approach & Evaluation
Several models will be developed and assessed starting with the simplest. Accuracy will be evaluated using the RMSE.  

$$RMSE=\sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i}-y_{u,i})^2}$$
              
N is defined as the number of user/movie combination,$Y_{u,i}$ as the rating for movie i by user u with the prediction as $\hat{Y}_{u,i}$.
To compute RMSE, use the loss function to calculate the residual (difference between prediction and truth) for each data point (rating). 
Basically the loss function computes the RMSE as the measure of accuracy for the error in rating. 

For this project, if the number is larger than 1 it means the typical error is larger than one star. The goal is to reduce the error below **0.8649.**
Accuracy will be compared across all models using the Loss function below:

```{r Loss Function, echo=TRUE, message=FALSE, warning=FALSE}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}
```
# Model 1 - A Naive Model Approach
The first model is the simplest recommendation system by assuming the same rating (the average) for all movies regardless of user 
and all differences were assumed to be random variation around this "true" rate. 

$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$
$\varepsilon_{u,i}$ as the independent sample errors and $\mu$ as the true rating for all movies.
```{r mu_hat, echo=TRUE, message=FALSE, warning=FALSE}

  mu_hat <- mean(train$rating)
  mu_hat
  
```
In this case the lowest RMSE would be attained using the observed
average of the data set as the estimate, with a resulting RMSE of:

```{r Naive RMSE, echo=TRUE, message=FALSE, warning=FALSE}
mu_hat <- mean(train$rating)
naive_rmse <- RMSE(test$rating, mu_hat)
naive_rmse
```
This is the first RMSE. Different approaches will be compared. Let's create a result table with this first RMSE. 
```{r RMSE Results Model 1, echo=FALSE, fig.height=2, fig.width=4, message=FALSE, warning=FALSE, paged.print=FALSE}
rmse_results <- tibble(Method = "Model 1 - Naive (Observed Average)", 
                         RMSE = naive_rmse)
#rmse_results %>% knitr::kable()
rmse_results %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```
This typical error is greater than 1 star. It was our first simple attempt that lacks accuracy. Our next model will build on this. 

# Model 2 - A Movie Effects Approach
We know from experience and data confirms this, that some movies are more popular than others and receive higher ratings. 
Taking into account the bias effects associated with the movies, we can add the $b_i$ to the existing model to reflect the bias.  

$b_i$ represents the average effect for movie $i$:

$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

This distribution shows the bias. The mean is at 0 so a $b_i$ of 1.5 reflects a 5 star rating.
```{r Movie Effects Graph, echo=TRUE, fig.height=5, fig.width=7, message=FALSE, warning=FALSE}

 mu <- mean(train$rating)
  
 # bi is the movies averages
 bi <- train %>% 
    group_by(movieId) %>%
    summarize(bi = mean(rating - mu))
  
 bi %>% ggplot(aes(bi)) +
  geom_histogram(color = "black", fill = "deepskyblue2", bins = 10) +
  xlab("Movie Bias") +
  ylab("Count") +
  theme_bw()
```

When we account for the movie effect in the model, the RMSE is reduced.

```{r Movie Effects, echo=TRUE, message=FALSE, warning=FALSE}
 predicted_ratings <- mu + test %>%
    left_join(bi, by='movieId') %>%
    pull(bi)
  
  bi_rmse <- RMSE(predicted_ratings, test$rating)
  bi_rmse
  
   
```

```{r Model 2, echo=FALSE, message=FALSE, warning=FALSE}
# Record a 2nd rmse into the results table
  rmse_results <- bind_rows(rmse_results,
                            tibble(Method = "Model 2 - Movie Effects",  
                                     RMSE = bi_rmse))
  #rmse_results %>% knitr::kable()
  rmse_results %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

# Model 3 - A User Effects Approach
There is substantial variability across users as well.Some users are very cranky and tend to rate negatively and others love every movie and rate more positively. 
Taking into account the bias effects associated with the user, we can add the $b_u$ to the existing model to reflect the bias.

$$ Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i} $$
When we account for both the movie and user effects in the model, the RMSE is reduced further.  
 
```{r User Effects, echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
#bu is the user averages.
bu <- train %>% 
    left_join(bi, by="movieId") %>%
    group_by(userId) %>%
    summarize(bu = mean(rating - mu - bi))


  predicted_ratings <- test %>%  
    left_join(bi, by='movieId') %>% 
    left_join(bu, by='userId') %>% 
    mutate(pred = mu + bi + bu) %>% 
    pull(pred)
 
  bu_rmse <-  RMSE(predicted_ratings, test$rating)
  bu_rmse

 
```

```{r Model 3, echo=FALSE, message=FALSE, warning=FALSE}
  #Record a 3nd rmse into the results table
  rmse_results <- bind_rows(rmse_results,
                            tibble(Method = "Model 3 - User Effects",  
                                     RMSE = bu_rmse))
  rmse_results %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

# Model 4 - Regularization 
Some of the data is noisy. For example ratings on obscure or niche movies by only a few users. This adds variability and can increase RMSE. 
We can use regularization to avoid over fitting and penalize large estimates formed by small sample sizes to reduce this effect. 
The optimal value of $\lambda$ is determined through cross validation and applied to our model.

$$
    \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
    \lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
$$ 
This code displays the RMSE associated with various values of $\lambda$.

```{r Regularization with Lambda, echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train$rating)
  bi <- train %>% 
    group_by(movieId) %>% 
    summarize(bi = sum(rating - mu)/(n()+l))
  bu <- train %>% 
    left_join(bi, by="movieId") %>% 
    group_by(userId) |>
    summarize(bu = sum(rating - bi - mu)/(n()+l))
  predicted_ratings <- 
    test %>% 
    left_join(bi, by = "movieId") %>% 
    left_join(bu, by = "userId") %>% 
    mutate(pred = mu + bi + bu) %>% 
    pull(pred)
    return(RMSE(predicted_ratings, test$rating))
})

```

This Graph shows a range of lambdas VS RMSE. The optimal setting provides the lowest error.

```{r Lambdas Graph, echo=TRUE, fig.height=5, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
qplot(lambdas, rmses, color=I("blue"))+
theme_bw()

```

The value of lambda which results in lowest RMSE is:

```{r Lowest Lambda, echo=TRUE, message=FALSE, warning=FALSE}

  lambda <- lambdas[which.min(rmses)]
  lambda
```

The RMSE has improved however it is a very small gain in accuracy.
```{r Model 4 RMSE, echo=FALSE, message=FALSE, warning=FALSE}
 #Record a 4th rmse into the results table
rmse_results <- bind_rows(rmse_results,
                            tibble(Method = "Mdoel 4 - Regularized Movie + User Effects",  
                                   RMSE = min(rmses)))
#rmse_results %>% knitr::kable()
rmse_results %>%
kbl() %>%
kable_paper("hover", full_width = F)
```

Let's try another approach. 

# Model 5 Recommender Systems
Recommender systems use historical data to make predictions. It is based on historical behavior by its users. So far we have approached a dataset that features sparsity and biases with models that account for these effects with decent accuracy. To get better results we turn to a more advanced method called matrix factorization. Our user data is processed as a large and sparse matrix, then decomposed into two smaller dimensional matrices with latent features and less sparsity. To make the process more efficient the recosystem packgage will be used. For more information on [Recommender System Using Parallel Matrix Factorization](https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html). 
We start by converting data into the recosystem format, finding the best tuning parameters, training,and finally testing it.

**Be advised this process may take from 4 to more than 30 minutes to run.**
```{r reconsystem, echo=TRUE, message=FALSE, warning=FALSE}
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")

  library(recosystem)
  # data_memory(): Specifies a data set from R objects
  set.seed(1, sample.kind="Rounding")
  train_reco <- with(train, data_memory(user_index = userId, item_index = movieId, rating = rating))
  test_reco <- with(test, data_memory(user_index = userId, item_index = movieId, rating = rating))
  
  # Create a model object (a Reference Class object in R) by calling Reco().
  r <- Reco()
  
  # select best tuning parameters along a set of candidate values
  para_reco <- r$tune(train_reco, opts = list(dim = c(20, 30),
                                              costp_l2 = c(0.01, 0.1),
                                              costq_l2 = c(0.01, 0.1),
                                              lrate = c(0.01, 0.1),
                                              nthread = 4,
                                              niter = 10))
  
  # Train the model 
  r$train(train_reco, opts = c(para_reco$min, nthread = 4, niter = 30))
  # Compute predicted values 
  results_reco <- r$predict(test_reco, out_memory())
```
 
 With the algorithm trained now we can test to see the resulting RMSE. 
 
```{r Test the Trained, echo=TRUE, message=FALSE, warning=FALSE}
factorization_rmse <- RMSE(results_reco, test$rating)
```
 
```{r Model 5, echo=FALSE, message=FALSE, warning=FALSE}
# Record our 5th Model
rmse_results <- bind_rows(rmse_results, tibble(Method = "Model 5: Matrix Factorization Using Recosystem", RMSE = factorization_rmse))
rmse_results %>%
kbl() %>%
kable_paper("hover", full_width = F)
```

This is a great improvement. The RMSE is significantly less than the target RMSE. **We have our model.**  

```{r Clear Objects, message=FALSE, warning=FALSE, include=FALSE}
  # clear objects from memory 
  rm (train_reco, para_reco, results_reco)
```

# Final Validation 
Now that we've found the lowest RMSE, the final step is to use matrix factorization to train it using the edx dataset and
then test its accuracy on the validation set. 

**Be advised this process may take from 20 to more than an hour to run.**
```{r Train the Final Model , echo=TRUE, message=FALSE, warning=FALSE}
set.seed(1, sample.kind="Rounding")
edx_reco <- with(edx, data_memory(user_index = userId, item_index = movieId, rating = rating))
validation_reco <- with(validation, data_memory(user_index = userId, item_index = movieId, rating = rating))
  
r <- Reco()
  
#Be advised, this may take more than 45 min to run: 
para_reco <- r$tune(edx_reco, opts = list(dim = c(20, 30),
                                            costp_l2 = c(0.01, 0.1),
                                            costq_l2 = c(0.01, 0.1),
                                            lrate = c(0.01, 0.1),
                                            nthread = 4,
                                            niter = 10))

r$train(edx_reco, opts = c(para_reco$min, nthread = 4, niter = 30))

final_reco <- r$predict(validation_reco, out_memory())
```

```{r get Final RMSE, message=FALSE, warning=FALSE}
final_rmse <- RMSE(final_reco, validation$rating)
```
  
  
```{r Validate Accuracy, echo=FALSE, message=FALSE, warning=FALSE}
 # Record our final model
 rmse_results <- bind_rows(rmse_results, tibble(Method = "Final Validation: Matrix factorization using recosystem", RMSE = final_rmse))
 rmse_results %>%
 kbl() %>%
 kable_paper("hover", full_width = F)
```
```{r clear final memory, message=FALSE, warning=FALSE, include=FALSE}
 rm (para_reco, final_reco)
```

# Conclusion

The final RMSE is 0.7805. Significantly below the target of 0.8649. We developed and tested several models and reached an acceptable RMSE with the regularized model and achieved a better accuracy using matrix factorization which was simplified through the recosystem package.

There are many advantages of using matrix factorization. It is one of the most sought-after machine learning recommendation models. It is a powerful technique for user or item-based collaborative filtering machine learning which was used to quantify residuals within the error loss based on patterns observed between groups of movies or users such that the residual error in predictions could be further reduced (Irizarry 2020; Koren, Bell, and Volinsky 2009). Additionaly it is both scalable and compact which makes it memory efficient and compatible to use on personal computers. As a result, it has contributed to its popularity in reccommendation systems (Koren, Bell, and Volinsky 2009).

## Limitations

Machine learning as well as deep learning involve the use of large amounts of data and complex algorithms that require powerful computation hardware or new hardware that offers faster computation with a fraction of the energy usage. 

It took me several iterations to run the algorithm. Each iteration, with my laptop hardware capacity of i7 Intel Core at 1.90/2.11 GHz CPU and 16GB RAM, took me over an hour to complete, especially with matrix factorization and with ~9M data points to process. With optimal hardware and software capacity, we can use so many other advanced machine learning tools to accurately predict outcomes.   

## Future Work

Machine learning algorithms can only be as good as the data used to train them. Although the developed machine learning algorithm met the project objective, the movie recommendation system can be further evaluated for potential improvement if genres and date were included in the models, if additional information were available including the actual ratings, gender and age of of the raters, the years of the ratings all of which may influence the rating outcome.  

This project's dilemma is the “cold start” problem. Theoretically, the algorithm would have to be re-run each time a new user or movie is added to the dataset.

## References

    10M MovieLensDataset Version, http://files.grouplens.org/datasets/movielens/ml-10m.zipl. 
 
    2023 GroupLens, MovieLens 10M Dataset,https://grouplens.org/datasets/movielens/10m/l.
    
    Irizarry, Rafael A., Introduction to Data Science, Recommendation systems, last updated Dec 12, 2022,
    http://rafalab.dfci.harvard.edu/dsbook/large-datasets.html#user-effects.
    

    Koren, Y., R. Bell, and C. Volinsky. 2009. “Matrix Factorization Techniques for Recommender Systems.”,
    https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf.
    
    Chen, Edwin, Winning the Netflix Prize: A Summary,http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/l.
 
    Lange, Carsten. How to Create an R Markdown Research Report,https://www.youtube.com/watch?v=agFAR_EmXtw.
 
    Recommender System Using Parallel Matrix Factorization,
    https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.htmll.
 
    Zewe, Adam, MIT News Office, July 28, 2022, New hardware Offer Faster Computation for Artificial Intelligence, with Much Less
    Energy, https://news.mit.edu/2022/analog-deep-learning-ai-computing-0728.
 
    The R Script, R Markdown, and HTML and PDF output reports of this project can be accessed via Github, 
    https://github.com/STEMenerChi/DataScience/tree/main/HarvardXMovieLensProject.
    
